# ğŸ§  Mi Open WebUI

Una interfaz de chat web completa que conecta con [Ollama](https://ollama.com) para utilizar modelos LLM locales (como llama3, mistral, gemma, etc). Incluye backend en Node.js (Express) y frontend en React (Vite).

---

## âœ¨ Estructura del proyecto

```
openWebUI/
â”œâ”€â”€ client/                 # Frontend Vite + React
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ vite.config.js
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.jsx
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ components/
â”‚   â”‚       â”œâ”€â”€ ChatBox.jsx
â”‚   â”‚       â”œâ”€â”€ AssistantSelector.jsx
â”‚   â”‚       â””â”€â”€ FileUploader.jsx
â”œâ”€â”€ server/                 # Backend Node.js + Express
â”‚   â”œâ”€â”€ index.js
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ .env                # Variables de entorno (puerto, etc)
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ start_openwebui.bat     # Script para Windows
â”œâ”€â”€ start_openwebui.sh      # Script para Linux/macOS
â””â”€â”€ README.md
```

---

## âœ… Requisitos

* Tener [Ollama](https://ollama.com) instalado y corriendo: `ollama run llama3`
* Node.js >= 18
* Git (si vas a clonar o subir a GitHub)

---

## â–¶ï¸ EjecuciÃ³n en Windows (sin Docker)

1. AsegÃºrate de tener Ollama corriendo:

   ```powershell
   ollama run llama3
   ```

2. Lanza todo con doble clic o desde PowerShell:

   ```powershell
   ./start_openwebui.bat
   ```

3. Abre tu navegador en: [http://localhost:5173](http://localhost:5173)

---

## â–¶ï¸ EjecuciÃ³n en Linux/macOS (sin Docker)

```bash
chmod +x ./start_openwebui.sh
./start_openwebui.sh
```

---

## ğŸ³ EjecuciÃ³n con Docker

```bash
docker-compose up --build
```

Abre tu navegador en: [http://localhost:5173](http://localhost:5173)

---

## ğŸ”™ Subir a GitHub

```bash
git init
git add .
git commit -m "Inicial"
gh repo create mi-open-webui --public --source=. --remote=origin
# (o usa GitHub Desktop para crear el repo si prefieres GUI)
git push -u origin main
```

---

## ğŸ”§ PersonalizaciÃ³n

* Cambia el modelo en el backend (`server/index.js`), por defecto usa `llama3`
* Puedes editar los asistentes, contexto RAG y lÃ³gica fÃ¡cilmente.

---

## ğŸ“¦ InstalaciÃ³n manual (si no usas los scripts)

```bash
cd server
npm install
cd ../client
npm install
npm run dev
```

Y en otro terminal:

```bash
cd server
npm run dev
```

---

## ğŸ“¤ API del backend

* `GET    /api/assistants`          â†’ Lista asistentes
* `POST   /api/assistants`          â†’ Crear/editar asistente
* `DELETE /api/assistants/:id`      â†’ Eliminar asistente
* `GET    /api/context`             â†’ Texto concatenado RAG
* `POST   /api/uploadContext`       â†’ Subir archivo .txt para contexto
* `POST   /api/generate`            â†’ Llama al modelo con el historial

---

## ğŸ¤ª Estado

âœ”ï¸ Funcionalidad bÃ¡sica terminada
ğŸ§ª Probado en Windows, Linux y Docker
ğŸ”œ Pendiente: persistencia, carga de modelos en UI, login opcional

---

## âœ¨ CrÃ©ditos

Hecho por Adam con â¤ï¸ usando Express, Vite y Ollama
